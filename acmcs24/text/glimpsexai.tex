\section{A  Glimpse of XAI} \label{sec:glimpsexai}

This section is to contain a description of related work on 
Basics of (non-symbolic) XAI.
%
Methods of XAI can be broadly categorized as those targeting feature
attribution and those targeting feature selection:

\begin{description} 
\item[Feature attribution:] %assign relative importance to features
%
The goal of feature attribution is to assign a score to each feature, 
measuring the feature's contribution to a given prediction. 
Examples of feature attribution methods are
LIME~\cite{guestrin-kdd16} and SHAP~\cite{lundberg-nips17}. 
Both methods attempt to learn scores for the features, 
but while LIME is based on iterative sampling, SHAP is based on
approximate computation of Shapley values~\cite{shapley53}. 
%
%RISE~\cite{Petsiuk-corr18} is a method that generates numerous masks 
%and applies them to the input image to measure the importance of each pixel.

\item[Feature selection:]
The purpose of feature selection is to identify a set of features
that is sufficient for the prediction, i.e. if the set of features is fixed to their
given values, then the prediction is known, independently of the values of the
other features. An example of a feature selection method is Anchors~\cite{guestrin-aaai18},
which is based on sampling.
\end{description}

% \paragraph{Hybrid approaches:} Saliency maps~\cite{muller-ieee-proc21}

% \paragraph{Intrinsic interpretability:} the (interpretable) model is the explanation 
%
Besides feature attribution and selection, another approach to explainability 
is intrinsic interpretability \cite{rudin-naturemi19,molnar-bk20,rudin-ss22}, 
where the ML model, due to being interpretable, serves as the explanation. 
%
For example, this is the case of decision trees~\cite{breiman-ss01}, 
decision sets~\cite{leskovec-kdd16}, or other graph-based ML models~\cite{HuangII021}, 
which are considered to be naturally interpretable. 
In such cases, some sort of manual inspection of the model itself produces 
an explanation for the prediction.
%
Despite the apparent simplicity of the models, recent 
works~\cite{IzzaIM22,MarquesSilvaI23} have
demonstrated that even intrinsic interpretable models should be explained
(at least in terms of obtaining shorter and succinct explanations).

\begin{itemize}
\item Decision trees
\item Rule-based models: DL, DS, etc.
\end{itemize}
