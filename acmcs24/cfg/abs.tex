\begin{abstract}
  With the rapid advances in artificial intelligence (AI) in recent
  years, it has become critical to devise solutions to achieve
  trustworthy AI. One such solution is to enable machine learning (ML)
  models, and systems of artificial intelligence (AI) in general, with
  the ability to explain their operation. This area of research is
  generally referred to as eXplainable AI (XAI), and it has been the
  subject of massive interest in recent years.
  %
  However, most works on XAI are based on methods of non-symbolic AI,
  and exhibit critical shortcomings. These shortcomings can be shown
  to represent a variety of sources of error. Evidently, a credible
  foundation of trustworthy AI cannot be based on methods that can and
  often do produce erroneous results.
  %
  It is therefore apparent that most existing methods of XAI are
  unsuitable as a foundation for trustworthy AI.
  %
  The natural alternative to approaches of XAI based on non-symbolic
  AI are methods of symbolic AI, aiming at delivering rigorous
  solutions of XAI.
  %
  The purpose of this paper is to provide an in-depth overview of the
  progress that has been observed in the in the last few
  years in the emerging field of symbolic XAI, but also to identify
  existing research challenges.
\end{abstract}
